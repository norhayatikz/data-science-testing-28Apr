{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/norhayatikz/data-science-testing-28Apr/blob/main/pandas_continued_hsbc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pSTtavXX8F6"
      },
      "source": [
        "# Pandas for Exploratory Data Analysis II \n",
        "\n",
        "Pandas is a very useful Python library for data manipulation and exploration. We have so much more to see!\n",
        "\n",
        "In this lesson, we'll continue exploring Pandas for EDA. Specifically: \n",
        "\n",
        "- Identify and handle missing values with Pandas.\n",
        "- Implement groupby statements for specific segmented analysis.\n",
        "- Use apply functions to clean data with Pandas.\n",
        "- Concatenating objects with `.append()` and `.concat()`\n",
        "- Combining objects with `.join()` and `.merge()`\n",
        "- Combining timeseries objects with `.merge_ordered()`\n",
        "\n",
        "\n",
        "We'll implicitly review many functions from our first Pandas lesson along the way!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuQrMiJkX8GN"
      },
      "source": [
        "## Remember the AdventureWorks Cycles Dataset?\n",
        "<img align=\"right\" src=\"http://lh6.ggpht.com/_XjcDyZkJqHg/TPaaRcaysbI/AAAAAAAAAFo/b1U3q-qbTjY/AdventureWorks%20Logo%5B5%5D.png?imgmax=800\">\n",
        "\n",
        "Here's the Production.Product table [data dictionary](https://www.sqldatadictionary.com/AdventureWorks2014/Production.Product.html), which is a description of the fields (columns) in the table (the .csv file we will import below):<br>\n",
        "- **ProductID** - Primary key for Product records.\n",
        "- **Name** - Name of the product.\n",
        "- **ProductNumber** - Unique product identification number.\n",
        "- **MakeFlag** - 0 = Product is purchased, 1 = Product is manufactured in-house.\n",
        "- **FinishedGoodsFlag** - 0 = Product is not a salable item. 1 = Product is salable.\n",
        "- **Color** - Product color.\n",
        "- **SafetyStockLevel** - Minimum inventory quantity.\n",
        "- **ReorderPoint** - Inventory level that triggers a purchase order or work order.\n",
        "- **StandardCost** - Standard cost of the product.\n",
        "- **ListPrice** - Selling price.\n",
        "- **Size** - Product size.\n",
        "- **SizeUnitMeasureCode** - Unit of measure for the Size column.\n",
        "- **WeightUnitMeasureCode** - Unit of measure for the Weight column.\n",
        "- **DaysToManufacture** - Number of days required to manufacture the product.\n",
        "- **ProductLine** - R = Road, M = Mountain, T = Touring, S = Standard\n",
        "- **Class** - H = High, M = Medium, L = Low\n",
        "- **Style** - W = Womens, M = Mens, U = Universal\n",
        "- **ProductSubcategoryID** - Product is a member of this product subcategory. Foreign key to ProductSubCategory.ProductSubCategoryID.\n",
        "- **ProductModelID** - Product is a member of this product model. Foreign key to ProductModel.ProductModelID.\n",
        "- **SellStartDate** - Date the product was available for sale.\n",
        "- **SellEndDate** - Date the product was no longer available for sale.\n",
        "- **DiscontinuedDate** - Date the product was discontinued.\n",
        "- **rowguid** - ROWGUIDCOL number uniquely identifying the record. Used to support a merge replication sample.\n",
        "- **ModifiedDate** - Date and time the record was last updated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uobMlwbNX8GQ"
      },
      "source": [
        "### Import Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.122598Z",
          "start_time": "2021-02-22T07:30:56.240573Z"
        },
        "id": "_zX2lkPLX8GR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np # used for linear algebra and random sampling amongst other things\n",
        "import seaborn as sns # plotting library\n",
        "import matplotlib.pyplot as plt # plotting library\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8gUVJCVX8GS"
      },
      "source": [
        "### Read in the dataset\n",
        "\n",
        "We are using the `read_csv()` method (and the `\\t` separator to specify tab-delimited columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.127125Z",
          "start_time": "2021-02-22T07:30:57.124717Z"
        },
        "id": "HJc-QJ0TX8GT"
      },
      "outputs": [],
      "source": [
        "# Read in the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.131774Z",
          "start_time": "2021-02-22T07:30:57.129477Z"
        },
        "id": "afu0qcetX8GU"
      },
      "outputs": [],
      "source": [
        "# Output the first 3 rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.136141Z",
          "start_time": "2021-02-22T07:30:57.133937Z"
        },
        "id": "lLEat5GBX8GU"
      },
      "outputs": [],
      "source": [
        "# Use the appropriate Pandas attribute to output the number of rows x cols\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UNlHyyLX8GV"
      },
      "source": [
        "### Reset our index (like last time)\n",
        "\n",
        "Let's bring our `ProductID` column into the index since it's the PK (primary key) of our table and that's where PKs belong as a best practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.140749Z",
          "start_time": "2021-02-22T07:30:57.138118Z"
        },
        "id": "0paMlT3vX8GY"
      },
      "outputs": [],
      "source": [
        "# Replace the auto-generated index with the ProductID column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L798RO4X8Gn"
      },
      "source": [
        "## Handling missing data\n",
        "\n",
        "Recall missing data is a systemic, challenging problem for data analysts and data scientists. Imagine conducting a poll, but some of the data gets lost, or you run out of budget and can't complete it! ðŸ˜®<br><br>\n",
        "\n",
        "\"Handling missing data\" itself is a broad topic. We'll focus on two components:\n",
        "\n",
        "- Using Pandas to identify we have missing data\n",
        "- Strategies to fill in missing data (known in the business as `imputation` or `imputing`)\n",
        "- The act of filling in missing data with Pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsxz3k82X8G3"
      },
      "source": [
        "### Identifying missing data\n",
        "\n",
        "Before *handling*, we must identify we're missing data at all!\n",
        "\n",
        "We have a few ways to explore missing data, and they are reminiscent of our Boolean filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.240722Z",
          "start_time": "2021-02-22T07:30:57.142743Z"
        },
        "id": "bALV6pLVX8Mh"
      },
      "outputs": [],
      "source": [
        "# True when data isn't missing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.243886Z",
          "start_time": "2021-02-22T07:30:56.259Z"
        },
        "id": "AhJg7uLsX8Mi"
      },
      "outputs": [],
      "source": [
        "# True when data is missing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1QNp1wBX8Mj"
      },
      "source": [
        "Now, we may want to see null values in aggregate. We can use `sum()` to sum down a given column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.245106Z",
          "start_time": "2021-02-22T07:30:56.262Z"
        },
        "id": "o_RZmEjJX8Mj"
      },
      "outputs": [],
      "source": [
        "# Here's a nice method chaining approach to get counts of missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-3x5E70X8Mk"
      },
      "source": [
        "Look! We've found missing values!\n",
        "\n",
        "How could this missing data be problematic for our analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWSZVL-nX8Mo"
      },
      "source": [
        "### Understanding missing data\n",
        "\n",
        "Finding missing data is the easy part! Determining way to do next is more complicated.\n",
        "\n",
        "Typically, we are most interested in knowing **why** we are missing data. Once we know what 'type of missingness' we have (the source of missing data), we can proceed effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nLfO1WTX8Mp"
      },
      "source": [
        "Let's first quantify how much data we are missing. Here is another implementation of `prod.isnull().sum()`, only wrapped with a `DataFrame` and some labels to make it a little more user-friendly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.246338Z",
          "start_time": "2021-02-22T07:30:56.267Z"
        },
        "id": "3jMKeOdEX8Mq"
      },
      "outputs": [],
      "source": [
        "# Or we can make things pretty as follows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHsMixlpX8Mq"
      },
      "source": [
        "### Filling in missing data\n",
        "\n",
        "How we fill in data depends largely on why it is missing (types of missingness) and what sampling we have available to us.\n",
        "\n",
        "We may:\n",
        "\n",
        "- Delete missing data altogether\n",
        "- Fill in missing data with:\n",
        "    - The mean of the column\n",
        "    - The median of the column\n",
        "    - A predicted amount based on other factors\n",
        "- Collect more data:\n",
        "    - Resample the population\n",
        "    - Follow up with the authority providing data that is missing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkJ5T0_5X8Mr"
      },
      "source": [
        "In our case, let's focus on handling missing values in `Color`. Let's get a count of the unique values in that column. We will need to use the `dropna=False` kwarg, otherwise the `pd.Series.value_counts()` method will not count `NaN` (null) values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.247621Z",
          "start_time": "2021-02-22T07:30:56.271Z"
        },
        "id": "jx2zY9GkX8Ms"
      },
      "outputs": [],
      "source": [
        "# Let's get a value count with the nulls included\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bytWOazBX8Mt"
      },
      "source": [
        "Ahoy! We have 248 nulls!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5_vOeXuX8Mt"
      },
      "source": [
        "Option 1: Drop the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.249298Z",
          "start_time": "2021-02-22T07:30:56.275Z"
        },
        "id": "wEcssqILX8Mu"
      },
      "outputs": [],
      "source": [
        "# Drops rows where any row has a missing value\n",
        "# This does not happen *in place*, so we are not actually dropping UNLESS overriding\n",
        "# the default of the inplace parameter for `dropna()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5x1ym7jX8My"
      },
      "source": [
        "**Important!** `pd.DataFrame.dropna()` and `pd.Series.dropna()` are very versatile! Let's look at the docs (Series is similar):\n",
        "\n",
        "```python\n",
        "Signature: pd.DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
        "Docstring:\n",
        "Remove missing values.\n",
        "\n",
        "See the :ref:`User Guide <missing_data>` for more on which values are\n",
        "considered missing, and how to work with missing data.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "axis : {0 or 'index', 1 or 'columns'}, default 0\n",
        "    Determine if rows or columns which contain missing values are\n",
        "    removed.\n",
        "\n",
        "    * 0, or 'index' : Drop rows which contain missing values.\n",
        "    * 1, or 'columns' : Drop columns which contain missing value.\n",
        "\n",
        "    .. deprecated:: 0.23.0: Pass tuple or list to drop on multiple\n",
        "    axes.\n",
        "how : {'any', 'all'}, default 'any'\n",
        "    Determine if row or column is removed from DataFrame, when we have\n",
        "    at least one NA or all NA.\n",
        "\n",
        "    * 'any' : If any NA values are present, drop that row or column.\n",
        "    * 'all' : If all values are NA, drop that row or column.\n",
        "thresh : int, optional\n",
        "    Require that many non-NA values.\n",
        "subset : array-like, optional\n",
        "    Labels along other axis to consider, e.g. if you are dropping rows\n",
        "    these would be a list of columns to include.\n",
        "inplace : bool, default False\n",
        "    If True, do operation inplace and return None.\n",
        "```\n",
        "\n",
        "**how**: This tells us if we want to remove a row if _any_ of the columns have a null, or _all_ of the columns have a null.<br>\n",
        "**subset**: We can input an array here, like `['Color', 'Size', 'Weight']`, and it will only consider nulls in those columns. This is very useful!<br>\n",
        "**inplace**: This is if you want to mutate (change) the source dataframe. Default is `False`, so it will return a _copy_ of the source dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEB4KOYZX8M1"
      },
      "source": [
        "To accomplish the same thing, but implement it on our entire dataframe, we can do the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.250680Z",
          "start_time": "2021-02-22T07:30:56.280Z"
        },
        "id": "3POzVJ8BX8M2"
      },
      "outputs": [],
      "source": [
        "# Drops all nulls from the Color column, but returns the entire dataframe \n",
        "# instead of just the Color column\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFE9vIQ8X8M3"
      },
      "source": [
        "Option 2: Fill in missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjpDZz0BX8M4"
      },
      "source": [
        "Traditionally, we fill missing data with a median, average, or mode (most frequently occurring). For `Color`, let's replace the nulls with the string value `NoColor`.\n",
        "\n",
        "Let's first look at the way we'd do it with a single column, using the `pd.Series.fillna()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.251908Z",
          "start_time": "2021-02-22T07:30:56.284Z"
        },
        "id": "eJYOhp52X8M6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnNnLmcGX8M7"
      },
      "source": [
        "Now let's see how we'd do it to the whole dataframe, using the `pd.DataFrame.fillna()` method. Notice the similar API between the methods with the `value` kwarg. Good congruent design, pandas development team! The full dataframe is returned, and not just a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.253154Z",
          "start_time": "2021-02-22T07:30:56.287Z"
        },
        "id": "z6PZZXA0X8M7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyba3WtuX8M8"
      },
      "source": [
        "But wait! There's more! We can reference any other data or formulas we want with the imputation (the value we fill the nulls with). This is very handy if you want to impute with the average or median of that column... or even another column altogether! \n",
        "\n",
        "Here is an example where we will the nulls of `Color` with the average value from the `ListPrice` column. This has no practical value in this application, but immense value in other applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.254354Z",
          "start_time": "2021-02-22T07:30:56.292Z"
        },
        "id": "WF3d075pX8M8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b1fTEeMX8M9"
      },
      "source": [
        "They're gone! Important points:\n",
        "\n",
        "- Don't forget to use the `inplace=True` kwarg to mutate the source dataframe (i.e. `save changes`). \n",
        "- It is helpful to not use `inplace=True` initially to ensure your code/logic is correct, prior to making permanent changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IcnSRmPX8M9"
      },
      "source": [
        "## Groupby Statements\n",
        "\n",
        "In Pandas, groupby statements are similar to pivot tables in that they allow us to segment our population to a specific subset.\n",
        "\n",
        "For example, if we want to know the average number of bottles sold and pack sizes per city, a groupby statement would make this task much more straightforward.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E04EMk5hX8M-"
      },
      "source": [
        "To think how a groupby statement works, think about it like this:\n",
        "\n",
        "- **Split:** Separate our DataFrame by a specific attribute, for example, group by `Color`\n",
        "- **Combine:** Put our DataFrame back together and return some _aggregated_ metric, such as the `sum`, `count`, or `max`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA4VtFZPX8M-"
      },
      "source": [
        "![](http://i.imgur.com/yjNkiwL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gptgUoi_X8M_"
      },
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feKXTQ8mX8M_"
      },
      "source": [
        "Let's group by `Color`, and get a count of products for each color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.255557Z",
          "start_time": "2021-02-22T07:30:56.299Z"
        },
        "id": "1lD8CcYTX8NA"
      },
      "outputs": [],
      "source": [
        "# Group by Color, giving the number of products of each color\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MrXU0fEX8NA"
      },
      "source": [
        "What do we notice about this output? Are all columns the same? Why or why not?\n",
        "\n",
        "We can see that the `.count()` method excludes nulls, and there is no way to change this with the current implementation:\n",
        "```python\n",
        "Signature: .count()\n",
        "Docstring: Compute count of group, excluding missing values\n",
        "```\n",
        "\n",
        "As a best practice, you should either:\n",
        "- fill in nulls prior to your .count(), or\n",
        "- use the PK (primary key) of the table, which is guaranteed non-null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.257157Z",
          "start_time": "2021-02-22T07:30:56.303Z"
        },
        "id": "OjgS-CruX8NB"
      },
      "outputs": [],
      "source": [
        "# Here we can use 'x' as a dummy placeholder for nulls, simply \n",
        "# to get consistent counts for all columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmJpO5PpX8NB"
      },
      "source": [
        "Let's find out the most expensive price for an item, by `Color`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLU_S5ZuX8NB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KELVNcNSX8NC"
      },
      "source": [
        "We can also do multi-level groupbys. This is referred to as a `Multiindex` dataframe. Here, we can see the following fields in a nested group by, with a count of Name (with nulls filled!); effectively giving us a count of the number of products for every unique Class/Style combination:\n",
        "\n",
        "- Class - H = High, M = Medium, L = Low\n",
        "- Style - W = Womens, M = Mens, U = Universal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZmY124-X8NC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3nmk14rX8NG"
      },
      "source": [
        "We can also use the `.agg()` method with multiple arguments, to simulate a `.describe()` method like we used before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd-82P8VX8NG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2yjg9RyX8NH"
      },
      "source": [
        "## Apply functions for column operations\n",
        "\n",
        "Apply functions allow us to perform a complex operation across an entire columns highly efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_fMgESX8NH"
      },
      "source": [
        "For example, let's say we want to change our colors from a word, to just a single letter. How would we do that?\n",
        "\n",
        "The first step is writing a function, with the argument being the value we would receive from each cell in the column. This function will mutate the input, and return the result. This result will then be _applied_ to the source dataframe (if desired)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.258167Z",
          "start_time": "2021-02-22T07:30:56.319Z"
        },
        "id": "UbS7wcSHX8NH"
      },
      "outputs": [],
      "source": [
        "# Output a list of all unique colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.259132Z",
          "start_time": "2021-02-22T07:30:56.321Z"
        },
        "id": "Clv1L-ufX8NJ"
      },
      "outputs": [],
      "source": [
        "# Create a function to convert color names to single letters representing those colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.260231Z",
          "start_time": "2021-02-22T07:30:56.324Z"
        },
        "id": "Jpwn63sTX8NJ"
      },
      "outputs": [],
      "source": [
        "# Invoke your newly created function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtLrYEm5X8NJ"
      },
      "source": [
        "Now we can _apply_ this function to our `pd.Series` object, returning the result (which we can use to overwrite the source, if we choose)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.261370Z",
          "start_time": "2021-02-22T07:30:56.327Z"
        },
        "id": "EeE_c31uX8NP"
      },
      "outputs": [],
      "source": [
        "# With apply\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEDDCahzX8NQ"
      },
      "source": [
        "The `pd.DataFrame.apply` implementation is similar, however it effectively 'scrolls through' the columns and passes each one sequentially to your function:\n",
        "\n",
        "```python\n",
        "Objects passed to the function are Series objects whose index is\n",
        "either the DataFrame index (axis=0) or the DataFrame columns (axis=1).\n",
        "```\n",
        "\n",
        "It should only be used when you wish to apply the same function to all columns (or rows) of your `pd.DataFrame` object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf3lQhVrX8NR"
      },
      "source": [
        "We can also use `pd.Series.apply()` with a **lambda expression**. This is an undeclared function and is commonly used for simple functions within the `.apply()` method. Let's use it to add $100 to our `ListPrice` column. Hey, baby needs new shoes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.262886Z",
          "start_time": "2021-02-22T07:30:56.331Z"
        },
        "id": "-fnIk7cgX8NR"
      },
      "outputs": [],
      "source": [
        "# Without apply - using a lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.264485Z",
          "start_time": "2021-02-22T07:30:56.334Z"
        },
        "id": "bKaJALasX8NS"
      },
      "outputs": [],
      "source": [
        "# And now with 100 more dollars!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBFKlHFgX8NS"
      },
      "source": [
        "Boom! Maybe financing that new boat wasn't such a bad idea after all!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2tnUpIsX8NS"
      },
      "source": [
        "**Your turn:** Identify one other column where we may want to write a new apply function, or use the one we just created for the purposes of cleaning up our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.266187Z",
          "start_time": "2021-02-22T07:30:56.338Z"
        },
        "id": "8Sao31dwX8NT"
      },
      "outputs": [],
      "source": [
        "# Identify a column to change\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.267382Z",
          "start_time": "2021-02-22T07:30:56.340Z"
        },
        "id": "vSmVtXKhX8Rr"
      },
      "outputs": [],
      "source": [
        "# Write a function to mutate that column (or columns) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.268429Z",
          "start_time": "2021-02-22T07:30:56.343Z"
        },
        "id": "tLmgAfBpX8Rs"
      },
      "outputs": [],
      "source": [
        "# Apply that function across the whole column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQtKO9gAX8Rs"
      },
      "source": [
        "---\n",
        "<h1>Joining Table with Pandas</h1>\n",
        "\n",
        "Pandas provides support for combining `Series`, `DataFrame` and even `xarray` (3 dimensional `DataFrame`s, formerly known in pandas v0.20.0 as `Panel`s) objects with various kinds of set logic for the indicies and relational algebra functionality in the case of join / merge-type operations. More simply stated, this allows you to combine `DataFrame`s.\n",
        "\n",
        "<!-- Overview -->\n",
        "<details>\n",
        "    <summary>Overview</summary>\n",
        "    <ul>\n",
        "        <li><b>In this session, we'll cover:</b></li>\n",
        "        <br>\n",
        "        <ul>\n",
        "            <li>Concatenating objects with <code>.append()</code> and <code>.concat()</code></li>\n",
        "            <li>Combining objects with <code>.join()</code> and <code>.merge()</code></li>\n",
        "            <li>Combining timeseries objects with <code>.merge_ordered()</code></li>\n",
        "            <li>Traditionally, this functionality is performed in a relational database, such as <a href=\"https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html#compare-with-sql-join\">SQL</a>. With pandas, you'll be able to perform the same operations - in python! The backend is <code>numpy</code>, a powerful linear algebra library which helps keep things speedy</li>\n",
        "        </ul>\n",
        "        <br>\n",
        "        <li><b>Why Join?</b></li>\n",
        "        <br>\n",
        "        <ul>\n",
        "            <li>You might be asking yourself - why keep data separated in different files? <i>Why not just keep it all in one file?</i></li>\n",
        "            <li>The answer stems from a thing called <a href=\"https://support.microsoft.com/en-us/help/283878/description-of-the-database-normalization-basics\">database normalization</a>. When a database is <i>normalized</i>, it is structured in such a way that redundancy of data is minimized. This allows a database to be faster, smaller, and more flexible when it comes time to change the data inside of it</li>\n",
        "            <li>The manifestation of this <i>normalization</i> is data that is represented within multiple <a href=\"https://en.wikipedia.org/wiki/Table_(database)\">tables</a> (which are effectively dataframes), related to each other by <a href=\"https://www.studytonight.com/dbms/database-key.php\">keys</a>, or columns in one table that equal a column in another table, allowing them to be joined. In this case, our tables are the <code>.csv</code> files we'll be importing</li>\n",
        "        </ul>\n",
        "    </ul>\n",
        "</details>\n",
        "\n",
        "<!-- TOC -->\n",
        "<details>\n",
        "    <summary>Table of Contents</summary>\n",
        "    <ul>\n",
        "        <li><a href=\"#conapp\">Concatenate and Append</a></li>\n",
        "        <ul>\n",
        "            <li><a href=\"#concatenate\">Concatenate</a></li>\n",
        "            <li><a href=\"#append\">Append</a></li>\n",
        "        </ul>\n",
        "        <li><a href=\"#joining\">Joining</a></li>\n",
        "        <ul>\n",
        "            <li><a href=\"join\">Join</a></li>\n",
        "            <li><a href=\"#merge\">Merge</a></li>\n",
        "            <ul>\n",
        "                <li><a href=\"#merge_keycols\">Merge on Non-Index Columns</a></li>\n",
        "                <li><a href=\"#yourturn\">Now it's Your Turn!</a></li>\n",
        "            </ul>\n",
        "        </ul>\n",
        "        <li><a href=\"#exercise\">Exercise - AdventureWorks</a></li>\n",
        "        <ul>\n",
        "            <li><a href=\"#p_exercise\">Table Joins on Live Data</a></li>\n",
        "            <ul>\n",
        "                <li><a href=\"#ex_pp\">Join Product Tables</a></li>\n",
        "                <li><a href=\"#ex_soh_sod\">Join Sales Order Header and Sales Order Detail Tables</a></li>\n",
        "                <li><a href=\"#ex_soh_sod_pt\">Join Sales Order Header, Sales Order Detail, and Product Tables</a></li>\n",
        "            </ul>\n",
        "        </ul>\n",
        "    </ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3X9GN8dX8Rt"
      },
      "source": [
        "<div id=\"conapp\"></div>\n",
        "<h2>Concatenate and Append</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-g52sGCX8Rt"
      },
      "source": [
        "<div id=\"concatenate\"></div>\n",
        "<h3>Concatenate</h3>\n",
        "\n",
        "Concatenate sticks dataframes together, either on top of each other, or next to each other.\n",
        "\n",
        "```python\n",
        "Signature: pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True)\n",
        "Docstring:\n",
        "Concatenate pandas objects along a particular axis with optional set logic\n",
        "along the other axes.\n",
        "```\n",
        "\n",
        "First, let's create two dataframes, `df1` and `df2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.274689Z",
          "start_time": "2021-02-22T07:30:56.444Z"
        },
        "id": "-BQosfi3X8Ru"
      },
      "outputs": [],
      "source": [
        "# KEEP\n",
        "df1 = pd.DataFrame([['a', 1], ['b', 2]], columns=['letter', 'number'])\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.276156Z",
          "start_time": "2021-02-22T07:30:56.446Z"
        },
        "id": "Z5dYrLhEX8Rz"
      },
      "outputs": [],
      "source": [
        "# KEEP\n",
        "df2 = pd.DataFrame([['c', 3], ['d', 4]], columns=['letter', 'number'])\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FANaZgReX8SC"
      },
      "source": [
        "Next, let's stick the dataframes on top of each other using `concat`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7ZjQT1FX8SI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcRtZIc-X8SL"
      },
      "source": [
        "Finally, let's stick the dataframes <b>next</b> to each other using `concat`. Use of the `axis` kwarg will help us here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsWAH5muX8SL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiYuHHPTX8SM"
      },
      "source": [
        "<div id=\"append\"></div>\n",
        "<h3>Append</h3>\n",
        "\n",
        "Append is very similar to `concat`, except it limits itself to a specific case of `concat`, where `axis=0` (stack on top of each other) and `join=outer` (how to handle the axis of the second dataframe). For almost all cases, `concat` has all the functionality of `append` (and more) and can replace `append` entirely.\n",
        "\n",
        "```python\n",
        "Signature: pd.DataFrame.append(self, other, ignore_index=False, verify_integrity=False, sort=None)\n",
        "Docstring:\n",
        "Append rows of `other` to the end of this frame, returning a new\n",
        "object. Columns not in this frame are added as new columns.\n",
        "```\n",
        "\n",
        "Also note that `append` is a DataFrame and Series method, and not a pandas library function like `concat` is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkQWXCNwX8SM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-XjEICIX8SN"
      },
      "source": [
        "<div id=\"joining\"></div>\n",
        "<h2>Joining</h2>\n",
        "\n",
        "<div id=\"join\"></div>\n",
        "<h3>Join</h3>\n",
        "\n",
        "`join` allows us to compare two dataframes, and combine them by using a matching column known as a `key`. Normally, during joins, this key is explicitly stated (we'll get to this with `merge` in our next example). With `join`, the `key` joining the table is always the `index` of the first table with (by default) the index of the second table. \n",
        "\n",
        "```python\n",
        "Signature: pd.DataFrame.join(self, other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
        "Docstring:\n",
        "Join columns with other DataFrame either on index or on a key\n",
        "column. Efficiently Join multiple DataFrame objects by index at once by\n",
        "passing a list.\n",
        "```\n",
        "\n",
        "First, let's create two dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.277550Z",
          "start_time": "2021-02-22T07:30:56.457Z"
        },
        "id": "mMPvXAnlX8SQ"
      },
      "outputs": [],
      "source": [
        "# KEEP\n",
        "df1 = pd.DataFrame([['a', 1], ['b', 2], ['c', 3], ['d', 4]], columns=['letter', 'number'])\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.279207Z",
          "start_time": "2021-02-22T07:30:56.459Z"
        },
        "id": "jvAeF4urX8SQ"
      },
      "outputs": [],
      "source": [
        "# KEEP\n",
        "df2 = pd.DataFrame([['e', 5], ['f', 6]], columns=['letter', 'number'])\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc97yWFbX8SR"
      },
      "source": [
        "Now, lets `join` these two dataframes. Note that we will `key`, or 'line up', the two dataframes based on their `indicies`.\n",
        "\n",
        "Note that, when joining dataframes with any common column names, we will need to supply a `lsuffix` or `rsuffix` kwarg. This is appended to the end of the column name of the returned, joined dataframe to differentiate and identify the source column. Here, we'll use `_df1` to identify that the column shown came from the `df1` dataframe, and `_df2` as a suffix to identify its origin as the `df2` dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEUWt9hgX8SR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTumDrmsX8SS"
      },
      "source": [
        "Note how we have joined the two dataframes on their indicies, which creates a null for rows of index 2 and 3 in `df2`. This is expected and correct.\n",
        "\n",
        "Also note that the default join behavior of `join` is `left`. We can change this with the `how` kwarg.\n",
        "\n",
        "For reference, here are the common types of joins. Join types won't be covered in this lesson.\n",
        "<p align=\"center\">\n",
        "<img width=\"500px\" src=\"https://i.stack.imgur.com/udQpD.jpg\">\n",
        "</p>\n",
        "\n",
        "The type of join we performed above is shown in the upper-left most figure in the above chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7_OXcuuX8SW"
      },
      "source": [
        "<div id=\"merge\"></div>\n",
        "<h3>Merge</h3>\n",
        "\n",
        "Similar to `join` is `merge`. The difference between the two is the <i>keying behavior</i>. `merge` has a richer API (more functionality) and allows one to join on columns in the source dataframe <i>other than the index</i>. Because `merge` can effectively do everything that `join` can do, and more - it is recommended to always use `merge` unless code brevity is the top concern. \n",
        "\n",
        "```python\n",
        "Signature: pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
        "Docstring:\n",
        "Merge DataFrame objects by performing a database-style join operation by\n",
        "columns or indexes.\n",
        "```\n",
        "\n",
        "Note that `merge` is <i>both</i> a DataFrame method as well as a pandas function. Below, we'll be using the pandas function, `pd.merge()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLHa2P8nX8Se"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnM51ZykX8Sf"
      },
      "source": [
        "Note that we've achieved the same exact output as we did with `join`, but it took a little more explicit work. Let's run through the arguments for clarity:\n",
        "\n",
        "<ul>\n",
        "    <li><code>df1</code>: this is the first dataframe, and considered to be on the 'left' of <code>df2</code></li>\n",
        "    <li><code>df2</code>: this is the second dataframe, considered to be on the right of <code>df1</code></li>\n",
        "    <li><code>how='left'</code>: this states the type of join; see the above SQL join table</li>\n",
        "    <li><code>left_index=True</code>: this uses the index of <code>df1</code> as the join key for the left table</li>\n",
        "    <li><code>right_index=True</code>: this uses the index of <code>df2</code> as the join key for the right table</li>\n",
        "    <li><code>suffixes</code>: this places <code>_df1</code> after column names which came from <code>df1</code></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL2ftqUVX8Sf"
      },
      "source": [
        "<div id=\"merge_keycols\"></div>\n",
        "<h4>Merge on Non-Index Columns</h4>\n",
        "\n",
        "This brings us to our next point: merging on columns that are not the index columns. This is very, very common in SQL joins and this technique can be used to join just about any DataFrame.\n",
        "\n",
        "First, let's create some more realistic data - stocks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.281080Z",
          "start_time": "2021-02-22T07:30:56.468Z"
        },
        "id": "zvgNgwlSX8Sg"
      },
      "outputs": [],
      "source": [
        "# KEEP\n",
        "openprice = pd.DataFrame({'Symbol': ['AAPL', 'DHR', 'DAL', 'AMZN'], \n",
        "                          'OpenPrice': [217.51, 96.54, 51.45, 1703.34]})\n",
        "wkhigh = pd.DataFrame({'Symbol': ['DAL', 'AMZN', 'AAPL', 'DHR'], \n",
        "                       '52wkHigh': [60.79, 2050.49, 233.47, 110.11]})\n",
        "stockname = pd.DataFrame({'Symbol': ['AMZN', 'DHR', 'DAL', 'AAPL'], \n",
        "                          'Name': ['Amazon', 'Danaher', 'Delta Airlines', 'Apple']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIgV5rBCX8Sj"
      },
      "source": [
        "Now, let's join the <code>openprice</code> and <code>wkhigh</code> dataframes together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdsnXK3NX8Sn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzoD4AzEX8Sn"
      },
      "source": [
        "Note how our `Symbol` column isn't in the same order in each dataframe. This is intentional, and note that the dataframe on the left, `openprice` dictates the order of the dataframe on the right, `wkhigh`. Also note that the shared key between the two dataframes is exempt from having a <code>suffix</code> applied to it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbkb3XL-X8So"
      },
      "source": [
        "<div id=\"yourturn\"></div>\n",
        "<h4>Now it's your turn!</h4>\n",
        "\n",
        "<ul>\n",
        "    <li><code>merge</code> the <code>openprice</code> and <code>stockname</code> dataframes and inspect the result</li>\n",
        "    <li><code>merge</code> all three dataframes together and inspect the result</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSxSbLOX8So"
      },
      "source": [
        "<div id=\"exercise\"></div>\n",
        "<h2>Exercise - Adventure Works</h2>\n",
        "<p align=\"right\">\n",
        "<img src=\"http://lh6.ggpht.com/_XjcDyZkJqHg/TPaaRcaysbI/AAAAAAAAAFo/b1U3q-qbTjY/AdventureWorks%20Logo%5B5%5D.png?imgmax=800\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ZNsrCoX8So"
      },
      "source": [
        "<div id=\"p_exercise\"></div>\n",
        "<h3>Table Joins on Live Data</h3>\n",
        "\n",
        "Here are the data dictionaries we'll be using for the following exercise:\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://www.sqldatadictionary.com/AdventureWorks2014/Production.Product.html\">Production.Product</a></li>\n",
        "    <li><a href=\"https://www.sqldatadictionary.com/AdventureWorks2014/Production.ProductSubCategory.html\">Production.ProductSubcategory</a></li>\n",
        "    <li><a href=\"https://www.sqldatadictionary.com/AdventureWorks2014/Sales.SalesOrderHeader.html\">Sales.SalesOrderHeader</a></li>\n",
        "    <li><a href=\"https://www.sqldatadictionary.com/AdventureWorks2014/Sales.SalesOrderDetail.html\">Sales.SalesOrderDetail</a></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-22T07:30:57.282761Z",
          "start_time": "2021-02-22T07:30:56.475Z"
        },
        "id": "huEqkcEDX8Sp"
      },
      "outputs": [],
      "source": [
        "p = pd.read_csv('Production.Product.csv', sep='\\t')\n",
        "ps = pd.read_csv('Production.ProductSubcategory.csv', sep='\\t')\n",
        "soh = pd.read_csv('Sales.SalesOrderHeader.csv', sep='\\t', nrows=1000)\n",
        "sod = pd.read_csv('Sales.SalesOrderDetail.csv', sep='\\t', nrows=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2lu2DfX8Sp"
      },
      "source": [
        "<div id=\"ex_pp\"></div>\n",
        "<h4>Join Product Tables</h4>\n",
        "\n",
        "<ul>\n",
        "    <li>Using the <code>Production.Product.ProductID</code> and <code>Production.ProductSubcategory.ProductID</code> keys, join the <code>Production.Product</code> and <code>Production.ProductSubcategory</code> tables</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2QZrnZCX8Sq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J7XNf6xX8Su"
      },
      "source": [
        "<div id=\"ex_soh_sod\"></div>\n",
        "<h4>Join Sales Order Header and Sales Order Detail Tables</h4>\n",
        "\n",
        "<ul>\n",
        "    <li>Join the <code>Sales.SalesOrderHeader</code> and <code>Sales.SalesOrderDetail</code> tables</li>\n",
        "    <li>Don't forget to use your data dictionaries!</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68w05eSqX8Sw"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dzXlcBUX8Sx"
      },
      "source": [
        "<div id=\"ex_soh_sod_pt\"></div>\n",
        "<h4>Join Sales Order Header, Sales Order Detail, and Product Tables</h4>\n",
        "\n",
        "<ul>\n",
        "    <li>Join the <code>Sales.SalesOrderHeader</code>, <code>Sales.SalesOrderDetail</code>, and <code>Production.Product</code> tables</li>\n",
        "    <li>Don't forget to use your data dictionaries!</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3u0aJb5X8Sy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pandas-continued-hsbc.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}